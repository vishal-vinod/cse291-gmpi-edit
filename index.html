<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>MPI Editing</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <meta property="og:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg"> -->
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <!-- <meta property="og:url" content="https://vishal-v.github.io/"> -->
    <meta property="og:title" content="Efficient Few-shot Identity Preserving Attribute Editing for 3D-aware Deep Generative Models">
    <meta property="og:description" content="Identity preserving editing of faces is a generative task that enables modifying the illumination, adding/removing eyeglasses, face aging, editing hairstyles, modifying expression etc., while preserving the identity of the face. Recent progress in 2D generative models have enabled photorealistic editing of faces using simple techniques leveraging the compositionality in GANs. However, identity preserving editing for 3D faces with a given set of attributes is a challenging task as the generative model must reason about view consistency from multiple poses and render a realistic 3D face. Further, 3D portrait editing requires large-scale attribute labelled datasets and presents a trade-off between editability in low-resolution and inflexibility to editing in high resolution. In this work, we aim to alleviate some of the constraints in editing 3D faces by identifying latent space directions that correspond to photorealistic edits. Recent 3D-aware methods utilizing the inherent semantic hierarchy for editing require semantic masks and training with expensive volumetric rendering at high resolutions with up-sampling that breaks multi-view consistency. While there has been research on conditional semantic space editing and efficient 3D-aware GAN inversion techniques, there has been minimal attention toward highly controlled identity preserving editing in 3D. To address this, we present a method that builds on recent advancements in 3D-aware deep generative models and 2D portrait editing techniques to perform efficient few-shot identity preserving attribute editing for 3D-aware generative models. We aim to show from experimental results that using just ten or fewer labelled images of an attribute is sufficient to estimate edit directions in the latent space that correspond to 3D-aware attribute editing. In this work, we leverage an existing face dataset with masks to obtain the synthetic images for few attribute examples required for estimating the edit directions. Further, to demonstrate the linearity of edits, we investigate one-shot stylization by performing sequential editing and use the (2D) Attribute Style Manipulation (ASM) technique to investigate a continuous style manifold for 3D consistent identity preserving aging.">

    <!-- <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Efficient Few-shot Identity Preserving Attribute Editing for 3D-aware Deep Generative Models">
    <meta name="twitter:description" content="Identity preserving editing of faces is a generative task that enables modifying the illumination, adding/removing eyeglasses, face aging, editing hairstyles, modifying expression etc., while preserving the identity of the face. Recent progress in 2D generative models have enabled photorealistic editing of faces using simple techniques leveraging the compositionality in GANs. However, identity preserving editing for 3D faces with a given set of attributes is a challenging task as the generative model must reason about view consistency from multiple poses and render a realistic 3D face. Further, 3D portrait editing requires large-scale attribute labelled datasets and presents a trade-off between editability in low-resolution and inflexibility to editing in high resolution. In this work, we aim to alleviate some of the constraints in editing 3D faces by identifying latent space directions that correspond to photorealistic edits. Recent 3D-aware methods utilizing the inherent semantic hierarchy for editing require semantic masks and training with expensive volumetric rendering at high resolutions with up-sampling that breaks multi-view consistency. While there has been research on conditional semantic space editing and efficient 3D-aware GAN inversion techniques, there has been minimal attention toward highly controlled identity preserving editing in 3D. To address this, we present a method that builds on recent advancements in 3D-aware deep generative models and 2D portrait editing techniques to perform efficient few-shot identity preserving attribute editing for 3D-aware generative models. We aim to show from experimental results that using just ten or fewer labelled images of an attribute is sufficient to estimate edit directions in the latent space that correspond to 3D-aware attribute editing. In this work, we leverage an existing face dataset with masks to obtain the synthetic images for few attribute examples required for estimating the edit directions. Further, to demonstrate the linearity of edits, we investigate one-shot stylization by performing sequential editing and use the (2D) Attribute Style Manipulation (ASM) technique to investigate a continuous style manifold for 3D consistent identity preserving aging.">
    <meta name="twitter:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg"> -->


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>Efficient Few-shot Identity Preserving Attribute Editing <br>
                    for 3D-aware Deep Generative Models<br>
                <small>
                    To be submitted to a conference.
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://vishal-v.github.io">
                              Vishal Vinod
                            </a>
                            <br>UC San Diego
                        </td>
                        <!-- <td>
                            <a style="text-decoration:none" href="https://phogzone.com/">
                              Peter Hedman
                            </a>
                            <br>Google Research
                        </td>
                        <td>
                            <a style="text-decoration:none" href="http://bmild.github.io/">
                             Ben Mildenhall
                            </a>
                            <br>Google Research
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="http://www.eecs.harvard.edu/~zickler/Main/HomePage">
                                Todd Zickler
                            </a>
                            <br>Harvard University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://jonbarron.info/">
                              Jonathan T. Barron
                            </a>
                            <br>Google Research
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://pratulsrinivasan.github.io/">
                              Pratul P. Srinivasan
                            </a>
                            <br>Google Research
                        </td> -->
                    </tr>
                </table>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <br/><br/>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://github.com/vishal-vinod/gmpi-edit">
                            <img src="./img/paper_image.png" height="60px">
                                <h4><strong>Paper (Coming Soon)</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="https://youtu.be/qrdRH9irAlk">
                            <img src="./img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li> -->
                        <li>
                            <a href="https://github.com/vishal-vinod/gmpi-edit" target="_blank">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>Synthetic Dataset (Coming Soon)</strong></h4>
                            </a>
                        </li>                     
                        <li>
                            <a href="https://github.com/vishal-vinod/gmpi-edit" target="_blank">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code (Coming Soon)</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="video-compare-container" id="materialsDiv">
                    <video class="video" id="materials" loop playsinline autoPlay muted src="video/materials_circle_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    
                    <canvas height=0 class="videoMerge" id="materialsMerge"></canvas>
                </div>
			</div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Identity preserving editing of faces is a generative task that enables modifying the illumination, adding/removing eyeglasses, face aging, editing hairstyles, modifying expression etc., while preserving the identity of the face. Recent progress in 2D generative models have enabled photorealistic editing of faces using simple techniques leveraging the compositionality in GANs. However, identity preserving editing for 3D faces with a given set of attributes is a challenging task as the generative model must reason about view consistency from multiple poses and render a realistic 3D face. Further, 3D portrait editing requires large-scale attribute labelled datasets and presents a trade-off between editability in low-resolution and inflexibility to editing in high resolution. In this work, we aim to alleviate some of the constraints in editing 3D faces by identifying latent space directions that correspond to photorealistic edits. Recent 3D-aware methods utilizing the inherent semantic hierarchy for editing require semantic masks and training with expensive volumetric rendering at high resolutions with up-sampling that breaks multi-view consistency. While there has been research on conditional semantic space editing and efficient 3D-aware GAN inversion techniques, there has been minimal attention toward highly controlled identity preserving editing in 3D. To address this, we present a method that builds on recent advancements in 3D-aware deep generative models and 2D portrait editing techniques to perform efficient few-shot identity preserving attribute editing for 3D-aware generative models. We aim to show from experimental results that using just ten or fewer labelled images of an attribute is sufficient to estimate edit directions in the latent space that correspond to 3D-aware attribute editing. In this work, we leverage an existing face dataset with masks to obtain the synthetic images for few attribute examples required for estimating the edit directions. Further, to demonstrate the linearity of edits, we investigate one-shot stylization by performing sequential editing and use the (2D) Attribute Style Manipulation (ASM) technique to investigate a continuous style manifold for 3D consistent identity preserving aging.
                </p>
            </div>
        </div>

        <!-- <image src="img/architecture.png" class="img-responsive" alt="overview" width="60%" style="max-height: 450px;margin:auto;"> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtube.com/embed/qrdRH9irAlk" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->
<!-- 
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Reflection Direction Parameterization
                </h3>
                <div class="text-justify">
                    Previous approaches directly input the camera's view direction into the MLP to predict outgoing radiance. We show that instead using the reflection of the view direction about the normal makes the emittance function significantly easier to learn and interpolate, greatly improving our results.
                    
                    <br><br>
                    
                </div>
                <div class="text-center">
                    <video id="refdir" width="40%" playsinline autoplay loop muted>
                        <source src="video/reflection_animation.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Integrated Directional Encoding
                </h3>
                <div class="text-justify">
                    We explicitly model object roughness using the expected values of a set of spherical harmonics under a von Mises-Fisher distribution whose concentration parameter varies spatially:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/ide.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    We call this <i>Integrated Directional Encoding</i>, and we show experimentally that it allows sharing the emittance functions between points with different roughnesses. It also enables scene editing after training. Theoretically, our encoding is stationary on the sphere, similar to the Euclidean stationarity of NeRF's positional encoding. <br><br>
                </div>
                <div class="text-center">
                    <video id="ide" width="100%" playsinline autoplay controls loop muted>
                        <source src="video/ide_animation.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Additional Synthetic Results
                </h3>
                <div class="video-compare-container">
                    <video class="video" id="musclecar" loop playsinline autoPlay muted src="video/musclecar_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="musclecarMerge"></canvas>
                </div>
			</div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results on Captured Scenes
                </h3>
                Our method also produces accurate renderings and surface normals from captured photographs:
                <div class="video-compare-container" style="width: 100%">
                    <video class="video" id="toycar" loop playsinline autoPlay muted src="video/toycar.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="toycarMerge"></canvas>
                </div>
			</div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Scene Editing
                </h3>
                <div class="text-justify">
                    We show that our structured representation of the directional MLP allows for scene editing after training. Here we show that we can convincingly change material properties.
                    <br>
                    We can increase and decrease material roughness:
                </div>

                <div style="overflow: hidden;">
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="margin-top: -5%;">
                        <source src="video/materials_rougher_smoother.mp4" type="video/mp4" />
                    </video>
                </div>

                <div class="text-justify">
                    We can also control the amounts of specular and diffuse colors, or change the diffuse color without affecting the specular reflections:
                </div>
                
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                            <video id="v2" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color2.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="50%">
                            <video id="v3" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color3.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>

            </div>
        </div> -->

            
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{verbin2022refnerf,
    title={{Ref-NeRF}: Structured View-Dependent Appearance for
           Neural Radiance Fields},
    author={Dor Verbin and Peter Hedman and Ben Mildenhall and
            Todd Zickler and Jonathan T. Barron and Pratul P. Srinivasan},
    journal={CVPR},
    year={2022}
}</textarea>
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We would like to thank Tanmay Shah, Dmitry Lagun, Tejan Karmali and Rishubh Parihar for helping us evaluate and discuss ideas and methods.
                The website template was borrowed from <a href="https://scholar.harvard.edu/dorverbin">Dor Verbin</a>.
                </p>
            </div>
        </div>
    </div>


</body></html>
